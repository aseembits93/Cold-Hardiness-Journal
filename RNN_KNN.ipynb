{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d1920c-42a1-492d-885e-29533a66b8ce",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f981de7-8da5-46ab-9183-88eb722583f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555d651-fc1c-4349-b2e8-accb6d2a57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the device for computation CPU/GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#random no for setting experiment\n",
    "runID = str(random.randint(11111, 99999))\n",
    "numberOfEpochs = 1#400\n",
    "batchSize = 12\n",
    "#Reading data\n",
    "all_data_path = \"./data/\"\n",
    "log_dir = os.path.join(\"./tensorboard/\", runID)\n",
    "use_saved_norm_constants = True\n",
    "cultivar = \"ColdHardiness_Grape_Merlot.csv\"\n",
    "cultivar_data_path = os.path.join(all_data_path, cultivar)\n",
    "df = pd.read_csv(cultivar_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2807b-bf36-4355-b536-92cd7e32f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # 'DATE', # date of weather observation\n",
    "    # 'AWN_STATION', # closest AWN station\n",
    "    # 'SEASON',\n",
    "    # 'SEASON_JDAY',\n",
    "    # 'DORMANT_SEASON',\n",
    "    # 'YEAR_JDAY',\n",
    "    # 'PHENOLOGY',\n",
    "    'PREDICTED_Hc',\n",
    "    # 'PREDICTED_Budbreak',\n",
    "    # mean temperature is the calculation of (max_f+min_f)/2 and then converted to Celsius. # they use this one\n",
    "    'MEAN_AT',\n",
    "    'MIN_AT',  # a\n",
    "    # 'AVG_AT', # average temp is AgWeather Network\n",
    "    'MAX_AT',  # a\n",
    "    'MIN_REL_HUMIDITY',  # a\n",
    "    'AVG_REL_HUMIDITY',  # a\n",
    "    'MAX_REL_HUMIDITY',  # a\n",
    "    'MIN_DEWPT',  # a\n",
    "    'AVG_DEWPT',  # a\n",
    "    'MAX_DEWPT',  # a\n",
    "    'P_INCHES',  # precipitation # a\n",
    "    'WS_MPH',  # wind speed. if no sensor then value will be na # a\n",
    "    'MAX_WS_MPH',  # a\n",
    "    # 'WD_DEGREE', # wind direction, if no sensor then value will be na\n",
    "    # 'LW_UNITY', # leaf wetness sensor\n",
    "    # 'SR_WM2', # solar radiation\n",
    "    # 'MIN_ST2',\n",
    "    # 'ST2',\n",
    "    # 'MAX_ST2',\n",
    "    # 'MIN_ST8',\n",
    "    # 'ST8', # soil temperature\n",
    "    # 'MAX_ST8',\n",
    "    # 'SM8_PCNT', # soil moisture import matplotlib.pyplot as plt@ 8-inch depth # too many missing values for merlot\n",
    "    # 'SWP8_KPA', # stem water potential @ 8-inch depth # too many missing values for merlot\n",
    "    # 'MSLP_HPA', # barrometric pressure\n",
    "    # 'ETO', # evaporation of soil water lost to atmosphere\n",
    "    # 'ETR' # ???\n",
    "]\n",
    "\n",
    "label = ['LT10', 'LT50', 'LT90']\n",
    "lt10_knn_path = \"./knn/LT10_rnn.pkl\"\n",
    "lt50_knn_path = \"./knn/LT50_rnn.pkl\"\n",
    "lt90_knn_path = \"./knn/LT90_rnn.pkl\"\n",
    "budbreak_knn_path = \"./knn/Budbreak_rnn.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82b119-dd79-4731-a4d2-a78bc73803e9",
   "metadata": {},
   "source": [
    "### Data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c645d0a-c0a8-4aa5-85d0-85cbc20671b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interp(x, y, missing_indicator, show=True):\n",
    "    y = np.array(y)\n",
    "    if (not np.isnan(missing_indicator)):\n",
    "        missing = np.where(y == missing_indicator)[0]\n",
    "        not_missing = np.where(y != missing_indicator)[0]\n",
    "    else:\n",
    "        # special case for nan values\n",
    "        missing = np.argwhere(np.isnan(y)).flatten()\n",
    "        all_idx = np.arange(0, y.shape[0])\n",
    "        not_missing = np.setdiff1d(all_idx, missing)\n",
    "\n",
    "    interp = np.interp(x, not_missing, y[not_missing])\n",
    "\n",
    "    if show == True:\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.title(\"Linear Interp. result where missing = \" +\n",
    "                  str(missing_indicator) + \"  Values replaced: \" + str(len(missing)))\n",
    "        plt.plot(x, interp)\n",
    "        # plt.show()\n",
    "\n",
    "    return interp\n",
    "\n",
    "\n",
    "def remove_na(column_name):\n",
    "    total_na = df[column_name].isna().sum()\n",
    "\n",
    "    df[column_name] = df[column_name].replace(np.nan, -100)\n",
    "    df[column_name] = linear_interp(\n",
    "        np.arange(df.shape[0]), df[column_name], -100, False)\n",
    "    if df[column_name].isna().sum() != 0:\n",
    "        assert False\n",
    "\n",
    "    print(\"Removed\", total_na, \"nan from\", column_name)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def get_before_after_phenology(max_season_len, season, df):\n",
    "    #print(season[0], season[-1])\n",
    "\n",
    "    _y = np.zeros((season_max_length, 1))\n",
    "    _y[:] = np.NaN\n",
    "\n",
    "    season_df = df[\"PHENOLOGY\"].iloc[season[0]:season[-1]]\n",
    "\n",
    "    budbreak_index = season_df.loc[season_df ==\n",
    "                                   \"Budburst/Budbreak\"].index.values\n",
    "\n",
    "    if (len(budbreak_index) == 1):\n",
    "        budbreak_index = budbreak_index[-1] - season[0]\n",
    "        #print(\"budbreak idx:\",  budbreak_index)\n",
    "        #print(\"_y before\", list(_y.flatten()))\n",
    "\n",
    "        _y[0:budbreak_index] = 0\n",
    "        _y[budbreak_index:] = 1\n",
    "\n",
    "        #print(\"_y after\", list(_y.flatten()))\n",
    "        #print(\"budbreak_index\", _y[budbreak_index])\n",
    "\n",
    "    return _y\n",
    "\n",
    "\n",
    "def split_and_normalize(_df, max_season_len, seasons, x_mean, x_std):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i, season in enumerate(seasons):\n",
    "        _x = (_df[features].loc[season, :]).to_numpy()\n",
    "\n",
    "        _x = np.concatenate(\n",
    "            (_x, np.zeros((season_max_length - len(season), len(features)))), axis=0)\n",
    "\n",
    "        add_array = np.zeros((season_max_length - len(season), len(label)))\n",
    "        add_array[:] = np.NaN\n",
    "\n",
    "        _y = _df.loc[season, :][label].to_numpy()\n",
    "        _y = np.concatenate((_y, add_array), axis=0)\n",
    "\n",
    "        _phen = get_before_after_phenology(\n",
    "            season_max_length, season, df)  # get before-after phen\n",
    "        # set before-after phen as another output dim\n",
    "        _y = np.concatenate((_y, _phen), axis=1)\n",
    "\n",
    "        #_y = np.reshape(_y, (_y.shape[0], _y.shape[1], _y.shape[2], 1))\n",
    "\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    norm_features_idx = np.arange(0, x_mean.shape[0])\n",
    "\n",
    "    x[:, :, norm_features_idx] = (\n",
    "        x[:, :, norm_features_idx] - x_mean) / x_std  # normalize\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#For budbreak, gets % gt < pred (higher is better)\n",
    "def _get_under(pred, gt):\n",
    "    total = 0\n",
    "    under = 0\n",
    "    for i in range(gt.shape[0]):\n",
    "        if (np.isnan(pred[i]) == False and np.isnan(gt[i]) == False):\n",
    "            #print(pred[i], gt[i])\n",
    "            total += 1\n",
    "\n",
    "            if (gt[i] < pred[i]):\n",
    "                under += 1\n",
    "\n",
    "    print(under, total, under / total * 100)\n",
    "    return under / total * 100\n",
    "\n",
    "#For budbreak\n",
    "def plot_budbreak(title, season, _y):\n",
    "    season_idx = np.arange(0, len(season))\n",
    "\n",
    "    add_array = np.zeros((season_max_length - len(season)))\n",
    "    add_array[:] = np.NaN\n",
    "\n",
    "    _y = np.concatenate((_y, add_array), axis=0)\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.scatter(x=season_idx, y=_y[:len(season)], label=\"Prediction\")\n",
    "\n",
    "    _phen = get_before_after_phenology(\n",
    "        252, season, df)  # get before-after phen\n",
    "\n",
    "    # draw vertical red line marking true budbreak if it exists\n",
    "    try:\n",
    "        true_budbreak_idx = np.where(_phen == 1)[0][0]\n",
    "        print(\"true_budbreak_idx:\", true_budbreak_idx)\n",
    "        print(\"predicted budbreak idx\", np.where(_y > 0.5)[0][0])\n",
    "        plt.vlines(x=true_budbreak_idx, ymin=0, ymax=5,\n",
    "                   colors='red', label='True Budbreak')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    plt.xlabel(\"Season Day\")\n",
    "    plt.ylabel(\"Budbreak\")\n",
    "\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    # plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5add5-9f4b-41df-bc0a-9123dffd6ad3",
   "metadata": {},
   "source": [
    "### Set up data for RNN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3412a7a-aec6-4754-9f2b-3b9000d94403",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_col in features:  # remove nan and do linear interp.\n",
    "    remove_na(feature_col)\n",
    "\n",
    "if False:\n",
    "    for f in features:\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.title(f)\n",
    "        plt.plot(np.arange(df.shape[0]), df[f])\n",
    "        plt.show()\n",
    "\n",
    "seasons = []\n",
    "last_x = 0\n",
    "idx = -1\n",
    "season_max_length = 0\n",
    "for x in df[df[\"DORMANT_SEASON\"] == 1].index.tolist():\n",
    "    if x - last_x > 1:\n",
    "        seasons.append([])\n",
    "        if idx > -1:\n",
    "            season_max_length = max(season_max_length, len(seasons[idx]))\n",
    "        idx += 1\n",
    "    seasons[idx].append(x)\n",
    "    last_x = x\n",
    "\n",
    "season_max_length = max(season_max_length, len(seasons[idx]))\n",
    "\n",
    "del seasons[19]  # drop season 2007 to 2008 [252 days] because of gap in temperature data \n",
    "\n",
    "print(\"len(seasons)\", len(seasons))\n",
    "print(\"Season lengths\", [len(x) for x in seasons])\n",
    "print(\"Max season length\", season_max_length)\n",
    "\n",
    "if False:\n",
    "    for i, season in enumerate(seasons):\n",
    "        _start = str(df[[\"DATE\"]].loc[season[0]].item().split(\"-\")[0])\n",
    "        _end = str(df[[\"DATE\"]].loc[season[-1]].item().split(\"-\")[0])\n",
    "        print(str(i) + \".\", _start + \" to \" + _end,\n",
    "              \"[\" + str(len(season)) + \" days]\")\n",
    "        #print(\"AVG_DEWPT\", list(df[\"AVG_DEWPT\"].iloc[season[0]:season[-1]] ))\n",
    "        _lt50 = np.array(df[\"LT50\"].iloc[season[0]:season[-1]])\n",
    "        print(\"n LT50\", np.count_nonzero(~np.isnan(_lt50)))\n",
    "if (use_saved_norm_constants == True):\n",
    "    try:\n",
    "        with open('normalizing_constants.pkl', 'rb') as _f:\n",
    "            norm_constants = pickle.load(_f)\n",
    "            x_mean = norm_constants[\"x_mean\"]\n",
    "            x_std = norm_constants[\"x_std\"]\n",
    "            print(\"restored constants from normalizing_constants.pkl. Constants were generated for model ID\",\n",
    "                  norm_constants[\"runID\"])\n",
    "    except:\n",
    "        print(\"normalizing_constants.pkl not found.\")\n",
    "        x_mean = df[features].mean().to_numpy()\n",
    "        x_std = df[features].std().to_numpy()\n",
    "else:\n",
    "    print(\"Not using normalizing_constants.pkl\")\n",
    "    x_mean = df[features].mean().to_numpy()\n",
    "    x_std = df[features].std().to_numpy()\n",
    "\n",
    "print(\"Normalizing Constants:\")\n",
    "print(\"x_mean\", x_mean)\n",
    "print(\"x_std\", x_std)\n",
    "\n",
    "# if state pickle does not exist, create it and quit\n",
    "if os.path.isfile('normalizing_constants.pkl') == False:\n",
    "    with open('normalizing_constants.pkl', 'wb') as _f:\n",
    "        norm_constants = {\n",
    "            \"runID\": runID,\n",
    "            \"features\": features,\n",
    "            \"x_mean\": x_mean,\n",
    "            \"x_std\": x_std\n",
    "        }\n",
    "        pickle.dump(norm_constants, _f)\n",
    "        print(\"Saved normalizing constants to file.\")\n",
    "\n",
    "x_train, y_train = split_and_normalize(\n",
    "    df, season_max_length, seasons[:-2 or None], x_mean, x_std)\n",
    "\n",
    "def get_not_nan(y):\n",
    "    return np.argwhere(np.isnan(y) == False)\n",
    "\n",
    "print(\"x_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"length of sequence\", len(x_train[0]))\n",
    "print(\"length of sequence\", len(y_train[0]))\n",
    "\n",
    "x_test, y_test = split_and_normalize(\n",
    "    df, season_max_length, seasons[-2:], x_mean, x_std)\n",
    "\n",
    "print(\"shape of test dataset\", x_test.shape)\n",
    "print(\"shape of test dataset\", y_test.shape)\n",
    "print(\"length of sequence\", len(x_test[0]))\n",
    "print(\"length of sequence\", len(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8270cba-efe9-4778-a7e2-2e7ce12ed676",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450537f2-8208-4d2f-9296-11cda73caad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training the RNN\n",
    "class net(nn.Module):  # LT50 and budbreak\n",
    "    def __init__(self, input_size):\n",
    "        super(net, self).__init__()\n",
    "\n",
    "        self.numLayers = 1\n",
    "\n",
    "        self.penul = 1024\n",
    "\n",
    "        self.memory_size = 2048\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.rnn = nn.GRU(input_size=2048, hidden_size=self.memory_size,\n",
    "                          num_layers=self.numLayers, batch_first=True)\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        self.linear3 = nn.Linear(self.memory_size, self.penul)  # penul\n",
    "        self.linear4 = nn.Linear(self.penul, 1)  # LT10\n",
    "        self.linear5 = nn.Linear(self.penul, 1)  # LT50\n",
    "        self.linear6 = nn.Linear(self.penul, 1)  # LT90\n",
    "        self.linear7 = nn.Linear(self.penul, 1)  # Budbreak\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        batch_dim, time_dim, state_dim = x.shape\n",
    "\n",
    "        out = self.linear1(x).relu()\n",
    "\n",
    "        #out = self.dropout(out)\n",
    "\n",
    "        out = self.linear2(out).relu()\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(self.numLayers, batch_dim,\n",
    "                            self.memory_size, device=x.device)\n",
    "\n",
    "        out, h_next = self.rnn(out, h)  # rnn out\n",
    "\n",
    "        out_s = self.linear3(out).relu()  # penul\n",
    "\n",
    "        out_lt_10 = self.linear4(out_s)  # LT10\n",
    "        out_lt_50 = self.linear5(out_s)  # LT50\n",
    "        out_lt_90 = self.linear6(out_s)  # LT90\n",
    "\n",
    "        out_ph = self.linear7(out_s).sigmoid()  # Budbreak\n",
    "\n",
    "        # return out_s, out_lt, out_ph, h_next\n",
    "        return out_lt_10, out_lt_50, out_lt_90, out_ph, h_next\n",
    "\n",
    "#For doing K-nearest neighbor search using RNN predictions (same weights but different output\n",
    "class knn_net(nn.Module): # LT50 and budbreak\n",
    "    def __init__(self, input_size):\n",
    "        super(knn_net, self).__init__()\n",
    "\n",
    "        self.numLayers = 1\n",
    "        \n",
    "        self.penul = 1024\n",
    "        \n",
    "        self.memory_size = 2048\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.rnn = nn.GRU(input_size=2048, hidden_size=self.memory_size, num_layers=self.numLayers, batch_first=True)\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        self.linear3 = nn.Linear(self.memory_size, self.penul) # penul\n",
    "        self.linear4 = nn.Linear(self.penul, 1) # LT10\n",
    "        self.linear5 = nn.Linear(self.penul, 1) # LT50\n",
    "        self.linear6 = nn.Linear(self.penul, 1) # LT90\n",
    "        self.linear7 = nn.Linear(self.penul, 1) # Budbreak\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        batch_dim, time_dim, state_dim = x.shape\n",
    "\n",
    "        out = self.linear1(x).relu()\n",
    "        \n",
    "        #out = self.dropout(out)\n",
    "    \n",
    "        out = self.linear2(out).relu()\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(self.numLayers, batch_dim, self.memory_size, device=x.device)\n",
    "\n",
    "        out, h_next = self.rnn(out, h) # rnn out\n",
    "        \n",
    "        out_s = self.linear3(out).relu() # penul\n",
    "\n",
    "        out_lt_10 = self.linear4(out_s) # LT10\n",
    "        out_lt_50 = self.linear5(out_s) # LT50\n",
    "        out_lt_90 = self.linear6(out_s) # LT90\n",
    "\n",
    "        out_ph = self.linear7(out_s).sigmoid() # Budbreak\n",
    "        \n",
    "        #return out_lt_10, out_lt_50, out_lt_90, out_ph, h_next\n",
    "        return out_s, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46989fc3-ee0e-446e-a11e-3ef3ddbed0f6",
   "metadata": {},
   "source": [
    "### Set up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcdb626-9c73-41b6-a913-94e92b543d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net(np.array(x_train).shape[-1])\n",
    "model.to(device)\n",
    "trainable_params = sum([np.prod(p.size()) for p in filter(\n",
    "    lambda p: p.requires_grad, model.parameters())])\n",
    "print(\"Trainable Parameters:\", trainable_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "criterion.to(device)\n",
    "criterion2 = nn.BCELoss()\n",
    "criterion2.to(device)\n",
    "writer = SummaryWriter(log_dir)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "trainLoader = DataLoader(train_dataset, batch_size=batchSize)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "val_dataset = TensorDataset(x_test, y_test)\n",
    "valLoader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449dc693-f327-4969-b839-4812259cb7e0",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecbd90-f9bc-47ee-8138-0d145d9577e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(numberOfEpochs):\n",
    "\n",
    "    # Training Loop\n",
    "    with tqdm(trainLoader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        tepoch.set_description(f\"Epoch {epoch + 1}/{numberOfEpochs} [T]\")\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for i, (x, y) in enumerate(trainLoader):\n",
    "            x_torch = x.to(device)\n",
    "            y_torch = y.to(device)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            out_lt_10, out_lt_50, out_lt_90, out_ph, _ = model(x_torch)\n",
    "\n",
    "            optimizer.zero_grad()       # zero the parameter gradients\n",
    "            \n",
    "            n_nan = get_not_nan(y[:, :, 0])  # LT10/50/90 not NAN\n",
    "            loss_lt_10 = criterion(\n",
    "                out_lt_10[n_nan[0], n_nan[1]], y_torch[:, :, 0][n_nan[0], n_nan[1]][:, None])  # LT10 GT\n",
    "\n",
    "            n_nan = get_not_nan(y[:, :, 1])  # LT10/50/90 not NAN\n",
    "            loss_lt_50 = criterion(\n",
    "                out_lt_50[n_nan[0], n_nan[1]], y_torch[:, :, 1][n_nan[0], n_nan[1]][:, None])  # LT50 GT\n",
    "\n",
    "            n_nan = get_not_nan(y[:, :, 2])  # LT10/50/90 not NAN\n",
    "            loss_lt_90 = criterion(\n",
    "                out_lt_90[n_nan[0], n_nan[1]], y_torch[:, :, 2][n_nan[0], n_nan[1]][:, None])  # LT90 GT\n",
    "\n",
    "            n_nan = get_not_nan(y[:, :, 3])  # budbreak not NAN\n",
    "            loss_ph = criterion2(\n",
    "                out_ph[n_nan[0], n_nan[1]], y_torch[:, :, 3][n_nan[0], n_nan[1]][:, None])  # Budbreak GT\n",
    "\n",
    "            loss = loss_lt_10 + loss_lt_50 + loss_lt_90 + loss_ph\n",
    "\n",
    "            loss.backward()             # backward +\n",
    "            optimizer.step()            # optimize\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            tepoch.set_postfix(Train_Loss=total_loss / count)\n",
    "            tepoch.update(1)\n",
    "\n",
    "        writer.add_scalar('Train_Loss', total_loss / count, epoch)\n",
    "\n",
    "    # Validation Loop\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valLoader, unit=\"batch\") as tepoch:\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}/{numberOfEpochs} [V]\")\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for i, (x, y) in enumerate(valLoader):\n",
    "                x_torch = x.to(device)\n",
    "                y_torch = y.to(device)\n",
    "                count += 1\n",
    "                out_lt_10, out_lt_50, out_lt_90, out_ph, _ = model(x_torch)\n",
    "                #getting non nan values is slow right now due to copying to cpu, write pytorch gpu version\n",
    "                n_nan = get_not_nan(y[:, :, 0])  # LT10/50/90 not NAN\n",
    "                loss_lt_10 = criterion(\n",
    "                    out_lt_10[n_nan[0], n_nan[1]], y_torch[:, :, 0][n_nan[0], n_nan[1]][:, None])  # LT10 GT\n",
    "\n",
    "                n_nan = get_not_nan(y[:, :, 1])  # LT10/50/90 not NAN\n",
    "                loss_lt_50 = criterion(\n",
    "                    out_lt_50[n_nan[0], n_nan[1]], y_torch[:, :, 1][n_nan[0], n_nan[1]][:, None])  # LT50 GT\n",
    "\n",
    "                n_nan = get_not_nan(y[:, :, 2])  # LT10/50/90 not NAN\n",
    "                loss_lt_90 = criterion(\n",
    "                    out_lt_90[n_nan[0], n_nan[1]], y_torch[:, :, 2][n_nan[0], n_nan[1]][:, None])  # LT90 GT\n",
    "\n",
    "                n_nan = get_not_nan(y[:, :, 3])  # budbreak not NAN\n",
    "                loss_ph = criterion2(\n",
    "                    out_ph[n_nan[0], n_nan[1]], y_torch[:, :, 3][n_nan[0], n_nan[1]][:, None])  # Budbreak GT\n",
    "\n",
    "                loss = loss_lt_10 + loss_lt_50 + loss_lt_90 + loss_ph\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                tepoch.set_postfix(Val_Loss=total_loss / count)\n",
    "                tepoch.update(1)\n",
    "\n",
    "            writer.add_scalar('Val_Loss', total_loss / count, epoch)\n",
    "with torch.no_grad():\n",
    "    out_lt_10, out_lt_50, out_lt_90, out_ph, _ = model(x_torch)\n",
    "#out_lt = out_lt.numpy()\n",
    "#out_ph = out_ph.numpy()\n",
    "\n",
    "print(out_lt_10.shape, out_lt_50.shape, out_lt_90.shape, out_ph.shape)\n",
    "# print(out_lt)\n",
    "# print(out_ph)\n",
    "modelSavePath = \"./models/\"\n",
    "torch.save(model.state_dict(), modelSavePath + runID + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f8e24-85e3-415e-8573-6d2f35b58537",
   "metadata": {},
   "source": [
    "### Plotting Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559aa029-6712-46be-aefc-914a2dc02442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots residual LT to ground-truth\n",
    "def graph_residual(plot_dict, title, x, xlabel, ylabel):\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "\n",
    "    x = np.array(x)\n",
    "    width = 1\n",
    "    pos = 0\n",
    "    for key, value in plot_dict.items():\n",
    "\n",
    "        plt.bar(x + pos, value, width, label=key)\n",
    "        pos += width\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "    return\n",
    "\n",
    "# plots residual LT to ground-truth\n",
    "def graph_residual_lte(plot_dict, title, x, xlabel, ylabel):\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    x = np.array(x)\n",
    "    width = 1\n",
    "    pos = 0\n",
    "    for key, value in plot_dict.items():\n",
    "        \n",
    "        plt.bar(x + pos, value, width, label = key)\n",
    "        pos += width\n",
    "    \n",
    "    plt.ylim(-4, 4)\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    return\n",
    "def graph_residual_ferg(title, ferg_diff, results_diff):\n",
    "    ferg_diff = ferg_diff.flatten()\n",
    "    results_diff = results_diff.flatten()\n",
    "    \n",
    "    days = np.arange(0, 252)\n",
    "    plot_dict = {\n",
    "                \"Furguson\": ferg_diff,\n",
    "                \"knn\": results_diff,\n",
    "                }\n",
    "    graph_residual_lte(plot_dict, title, days, \"Days\", \"Temperature. (Deg. C)\")\n",
    "    return\n",
    "def graph_residual_tempdiff(title, results_diff):\n",
    "    results_diff = results_diff.flatten()\n",
    "    \n",
    "    days = np.arange(0, 252)\n",
    "    plot_dict = {\n",
    "                \"knn\": results_diff,\n",
    "                }\n",
    "    graph_residual_lte(plot_dict, title, days, \"Days\", \"Temperature. (Deg. C)\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746c57e-27f8-4047-bff7-02e7dc653e19",
   "metadata": {},
   "source": [
    "### Get predictions of Fergusen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8063b-06b6-4740-823f-ea2a530d1804",
   "metadata": {},
   "outputs": [],
   "source": [
    "ferg_model = []\n",
    "\n",
    "for i, season in enumerate(seasons):\n",
    "    if i == 23 or i == 24:  # last 2 seasons for testing\n",
    "        _y = df[[\"PREDICTED_Hc\"]].loc[season, :].to_numpy()\n",
    "\n",
    "        add_array = np.zeros((season_max_length - len(season), 1))\n",
    "        add_array[:] = np.NaN\n",
    "\n",
    "        _y = np.concatenate((_y, add_array), axis=0)\n",
    "\n",
    "        ferg_model.append(_y)\n",
    "\n",
    "print(\"ferg 2020\", ferg_model[0].shape)\n",
    "print(\"ferg 2021\", ferg_model[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3548d-2a1f-4bbd-9939-b04b4f974ea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot RNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51982308-c104-48e1-b707-55c41989c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = y_test.cpu().detach().numpy()\n",
    "gt_2020_lt10 = gt[0, :, 0]\n",
    "gt_2021_lt10 = gt[1, :, 0]\n",
    "\n",
    "gt_2020_lt50 = gt[0, :, 1]\n",
    "gt_2021_lt50 = gt[1, :, 1]\n",
    "\n",
    "gt_2020_lt90 = gt[0, :, 2]\n",
    "gt_2021_lt90 = gt[1, :, 2]\n",
    "\n",
    "print(\"gt 2020 LT10\", gt_2020_lt10.shape)\n",
    "print(\"gt 2021 LT10\", gt_2021_lt10.shape)\n",
    "\n",
    "print(\"gt 2020 LT50\", gt_2020_lt50.shape)\n",
    "print(\"gt 2021 LT50\", gt_2021_lt50.shape)\n",
    "\n",
    "print(\"gt 2020 LT90\", gt_2020_lt90.shape)\n",
    "print(\"gt 2021 LT90\", gt_2021_lt90.shape)\n",
    "\n",
    "results_2020_lt10 = out_lt_10[0].cpu().numpy().flatten()\n",
    "results_2021_lt10 = out_lt_10[1].cpu().numpy().flatten()\n",
    "print(\"results_2021_lt10.shape\", results_2021_lt10.shape)\n",
    "\n",
    "results_2020_lt50 = out_lt_50[0].cpu().numpy().flatten()\n",
    "results_2021_lt50 = out_lt_50[1].cpu().numpy().flatten()\n",
    "print(\"results_2021_lt50.shape\", results_2021_lt50.shape)\n",
    "\n",
    "results_2020_lt90 = out_lt_90[0].cpu().numpy().flatten()\n",
    "results_2021_lt90 = out_lt_90[1].cpu().numpy().flatten()\n",
    "print(\"results_2021_lt90.shape\", results_2021_lt90.shape)\n",
    "\n",
    "ferg_diff_2020 = gt_2020_lt50 - ferg_model[0].flatten()\n",
    "results_diff_2020_lt50 = gt_2020_lt50 - results_2020_lt50\n",
    "\n",
    "print(ferg_diff_2020.shape)\n",
    "print(results_diff_2020_lt50.shape)\n",
    "\n",
    "ferg_diff_2020 = ferg_diff_2020.flatten()\n",
    "results_diff_2020_lt50 = results_diff_2020_lt50.flatten()\n",
    "\n",
    "days = np.arange(0, 252)\n",
    "plot_dict = {\n",
    "    str(runID): (gt_2020_lt10 - results_2020_lt10).flatten(),\n",
    "}\n",
    "graph_residual(plot_dict, \"LT10 Same Day | 2020-2021 Season |  GT - Pred\",\n",
    "               days, \"Days\", \"Temperature. (Deg. C)\")\n",
    "\n",
    "days = np.arange(0, 252)\n",
    "plot_dict = {\n",
    "    str(runID): (gt_2021_lt10 - results_2021_lt10).flatten(),\n",
    "}\n",
    "graph_residual(plot_dict, \"LT10 Same Day | 2021 - * Season |  GT - Pred\",\n",
    "               days, \"Days\", \"Temperature. (Deg. C)\")\n",
    "\n",
    "days = np.arange(0, 252)\n",
    "plot_dict = {\n",
    "    \"Furguson\": ferg_diff_2020,\n",
    "    str(runID): results_diff_2020_lt50,\n",
    "}\n",
    "graph_residual(plot_dict, \"LT50 Same Day | 2020-2021 Season |  GT - Pred\",\n",
    "               days, \"Days\", \"Temperature. (Deg. C)\")\n",
    "\n",
    "ferg_diff_2021 = gt_2021_lt50 - ferg_model[1].flatten()\n",
    "results_diff_2021_lt50 = gt_2021_lt50 - results_2021_lt50\n",
    "\n",
    "# print(ferg_diff_2021.shape)\n",
    "# print(results_diff_2021_lt50.shape)\n",
    "\n",
    "ferg_diff_2021 = ferg_diff_2021.flatten()\n",
    "results_diff_2021_lt50 = results_diff_2021_lt50.flatten()\n",
    "\n",
    "days = np.arange(0, 252)\n",
    "plot_dict = {\n",
    "    \"Furguson\": ferg_diff_2021,\n",
    "    str(runID): results_diff_2021_lt50,\n",
    "}\n",
    "graph_residual(\n",
    "    plot_dict, \"LT50 Same Day | 2021 - * Season |  GT - Pred\", days, \"Days\", \"LT50\")\n",
    "\n",
    "days = np.arange(0, 252)\n",
    "plot_dict = {\n",
    "    str(runID): (gt_2020_lt90 - results_2020_lt90).flatten(),\n",
    "}\n",
    "graph_residual(plot_dict, \"LT90 Same Day | 2020-2021 Season |  GT - Pred\",\n",
    "               days, \"Days\", \"Temperature. (Deg. C)\")\n",
    "\n",
    "days = np.arange(0, 252)\n",
    "plot_dict = {\n",
    "    str(runID): (gt_2021_lt90 - results_2021_lt90).flatten(),\n",
    "}\n",
    "graph_residual(plot_dict, \"LT90 Same Day | 2021 - * Season |  GT - Pred\",\n",
    "               days, \"Days\", \"Temperature. (Deg. C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b18df5-9d87-47f9-9d96-556d89ecbe6f",
   "metadata": {},
   "source": [
    "### budbreak results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173824a-dfac-449b-b027-353dd1389e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_gt_2020_lt50 = gt[0, :, 1]\n",
    "bb_gt_2021_lt50 = gt[1, :, 1]\n",
    "\n",
    "bb_results_2020 = out_ph[0].cpu().numpy().flatten()\n",
    "bb_results_2021 = out_ph[1].cpu().numpy().flatten()\n",
    "\n",
    "#bb_results_2020 = np.where(bb_results_2020 > 0.5, 1, 0)\n",
    "#bb_results_2021 = np.where(bb_results_2021 > 0.5, 1, 0)\n",
    "\n",
    "plot_budbreak(\"Budbreak 2021-2022\", seasons[-2], bb_results_2020)\n",
    "plot_budbreak(\"Budbreak 2022-*\", seasons[-1], bb_results_2021)\n",
    "\n",
    "ferg_2020_under_rate = _get_under(ferg_model[0].flatten(), gt_2020_lt50)\n",
    "print(\"ferg_2020 % gt < pred\", ferg_2020_under_rate)\n",
    "\n",
    "model_2020_under_rate = _get_under(results_2020_lt50, gt_2020_lt50)\n",
    "print(\"model_2020 % gt < pred\", model_2020_under_rate)\n",
    "\n",
    "ferg_2021_under_rate = _get_under(ferg_model[1], gt_2021_lt50)\n",
    "print(\"ferg_2021 % gt < pred\", ferg_2021_under_rate)\n",
    "\n",
    "model_2021_under_rate = _get_under(results_2021_lt50, gt_2021_lt50)\n",
    "print(\"model_2021 % gt < pred\", model_2021_under_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaaa211-104b-49fd-bf84-9b7ea317dca0",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d2c81-0985-41d8-bff9-2a76ba863974",
   "metadata": {},
   "source": [
    "### data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475f718-6432-41a2-bb52-14f28feb27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_rdc(train_set, test_set, pca_thres=75, n_components=None):\n",
    "    from sklearn.decomposition import PCA\n",
    "    if n_components is None:\n",
    "        pca = PCA()\n",
    "        pca.fit(train_set)\n",
    "        idx_reduction = pca.explained_variance_ratio_ >= np.percentile(\n",
    "            pca.explained_variance_ratio_, pca_thres)\n",
    "        n_components = idx_reduction.sum()\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(train_set)\n",
    "\n",
    "    return pca.transform(train_set), pca.transform(test_set), n_components, pca\n",
    "\n",
    "\n",
    "def get_sets(x, y, ox, model, test_set=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_s, pred_rnn,  = model(x)\n",
    "\n",
    "        if test_set == False:\n",
    "            valuable_idx = get_not_nan(y)\n",
    "\n",
    "            gt = y[valuable_idx[0], valuable_idx[1]]  # ground truth\n",
    "\n",
    "            # penultimate layer before the last linear layer\n",
    "            _penultimate = pred_s[valuable_idx[0], valuable_idx[1]]\n",
    "\n",
    "            _rnn = pred_rnn[valuable_idx[0],\n",
    "                            valuable_idx[1]]  # rnn output layer\n",
    "        else:\n",
    "            gt = y.view(-1)\n",
    "\n",
    "            _penultimate = pred_s.reshape(\n",
    "                pred_s.size(0) * pred_s.size(1), pred_s.size(2))\n",
    "\n",
    "            _rnn = pred_rnn.reshape(pred_rnn.size(\n",
    "                0) * pred_rnn.size(1), pred_rnn.size(2))\n",
    "\n",
    "        return gt, _rnn, _penultimate\n",
    "\n",
    "def IDW_KNN(train_x, train_gt, test_x, test_gt=None, n_neighbors=17, inverse_exp=-1, criterion=None):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    # print(train_x.shape)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors,\n",
    "                            leaf_size=100).fit(train_x)\n",
    "    pred_distances, pred_indices = nbrs.kneighbors(test_x)\n",
    "\n",
    "    inverse_dist = pred_distances ** inverse_exp\n",
    "    # print(inverse_dist.shape)\n",
    "    norm_inverse_dist = inverse_dist / inverse_dist.sum(axis=1)[:, None]\n",
    "\n",
    "    weigted_value = torch.squeeze(train_gt[pred_indices]) * norm_inverse_dist\n",
    "    prediction = weigted_value.sum(axis=1).float()\n",
    "\n",
    "    if (test_gt != None):\n",
    "        loss = criterion(prediction, test_gt).item()\n",
    "    else:\n",
    "        loss = None\n",
    "\n",
    "    return prediction, loss, norm_inverse_dist, nbrs\n",
    "\n",
    "def knn_param_search(train_penul, test_penul, train_rnn, test_rnn, train_gt, test_gt, criterion, name=None):\n",
    "    lowest_penul = float('inf')\n",
    "    lowest_rnn = float('inf')\n",
    "\n",
    "    k_e_penul = (None, None, None, None)\n",
    "    k_e_rnn = (None, None, None, None)\n",
    "\n",
    "    best_penul_knn = None\n",
    "\n",
    "    best_rnn_knn = None\n",
    "\n",
    "    best_penul_pca = None\n",
    "\n",
    "    best_rnn_pca = None\n",
    "\n",
    "    for pca_thres in [30, 50, 75, 100]:\n",
    "        print(\"===================pca_thres: {}=====================\".format(pca_thres))\n",
    "        for j in range(-1, -6, -1):\n",
    "            tl = []\n",
    "            tr = []\n",
    "            x_axis = []\n",
    "\n",
    "            train_x_um_penul_rdc, test_x_um_penul_rdc, n_components_penul, pca_penul = pca_rdc(\n",
    "                train_penul, test_penul, pca_thres=pca_thres)\n",
    "\n",
    "            train_x_um_rnn_rdc, test_x_um_rnn_rdc, n_components_rnn, pca_rnn = pca_rdc(\n",
    "                train_rnn, test_rnn, pca_thres=pca_thres)\n",
    "\n",
    "            for i in range(2, 600, 5):\n",
    "\n",
    "                # 1) penultimate layer before the last linear layer\n",
    "                _, loss_penul, _, knn_penul = IDW_KNN(\n",
    "                    train_x_um_penul_rdc, train_gt, test_x_um_penul_rdc, test_gt, n_neighbors=i, inverse_exp=j, criterion=criterion)\n",
    "\n",
    "                # 2) RNN output\n",
    "                _, loss_rnn, _, knn_rnn = IDW_KNN(\n",
    "                    train_x_um_rnn_rdc, train_gt, test_x_um_rnn_rdc, test_gt, n_neighbors=i, inverse_exp=j, criterion=criterion)\n",
    "\n",
    "                tl.append(loss_penul)\n",
    "                tr.append(loss_rnn)\n",
    "                x_axis.append(i)\n",
    "\n",
    "                if loss_penul < lowest_penul:\n",
    "                    k_e_penul = (i, j, pca_thres, n_components_penul)\n",
    "                    lowest_penul = loss_penul\n",
    "                    best_penul_knn = knn_penul\n",
    "                    best_penul_pca = pca_penul\n",
    "                    print(\"Loss / k_e_penul\", lowest_penul, k_e_penul)\n",
    "\n",
    "                if loss_rnn < lowest_rnn:\n",
    "                    k_e_rnn = (i, j, pca_thres, n_components_rnn)\n",
    "                    lowest_rnn = loss_rnn\n",
    "                    best_rnn_knn = knn_rnn\n",
    "                    best_rnn_pca = pca_rnn\n",
    "                    print(\"Loss / k_e_rnn\", lowest_rnn, k_e_rnn)\n",
    "\n",
    "            # print(tl)\n",
    "            #_params = \"(n:\" + str(i) + \", inverse_exp:\" + str(j) + \")\"\n",
    "            # print(\"*********{}*********\".format(j))\n",
    "            # plt.cla()\n",
    "            #plt.plot(x_axis, tl, label = \"last\")\n",
    "            # plt.xlabel(\"K\")\n",
    "            # plt.ylabel(\"loss\")\n",
    "            #plt.title(\"Penultimate \" + _params)\n",
    "            # plt.show()\n",
    "            # plt.cla()\n",
    "            #plt.plot(x_axis, tr, label = \"rnn\")\n",
    "            # plt.xlabel(\"K\")\n",
    "            # plt.ylabel(\"loss\")\n",
    "            #plt.title(\"RNN \"+ _params)\n",
    "            # plt.show()\n",
    "            # plt.cla()\n",
    "\n",
    "    # Save best to pkl\n",
    "\n",
    "    best_penul = {\n",
    "        \"model\": model_path,\n",
    "        \"neighbors\": k_e_penul[0],\n",
    "        \"exponent\": k_e_penul[1],\n",
    "        \"pca_thres\": k_e_penul[2],\n",
    "        \"components\": k_e_penul[3],\n",
    "        \"loss\": loss_penul,\n",
    "        \"train_gt\": train_gt,\n",
    "        \"pca\": best_penul_pca,\n",
    "        \"knn\": best_penul_knn,\n",
    "    }\n",
    "\n",
    "    best_rnn = {\n",
    "        \"model\": model_path,\n",
    "        \"neighbors\": k_e_rnn[0],\n",
    "        \"exponent\": k_e_rnn[1],\n",
    "        \"pca_thres\": k_e_rnn[2],\n",
    "        \"components\": k_e_rnn[3],\n",
    "        \"loss\": lowest_rnn,\n",
    "        \"train_gt\": train_gt,\n",
    "        \"pca\": best_rnn_pca,\n",
    "        \"knn\": best_rnn_knn,\n",
    "    }\n",
    "\n",
    "    print(best_penul)\n",
    "\n",
    "    print(best_rnn)\n",
    "\n",
    "    if (best_penul['loss'] < best_rnn['loss']):\n",
    "        with open('./knn/' + name + '_penul.pkl', 'wb') as _f:\n",
    "            pickle.dump(best_penul, _f)\n",
    "    elif (best_rnn['loss'] < best_penul['loss']):\n",
    "        with open('./knn/' + name + '_rnn.pkl', 'wb') as _f:\n",
    "            pickle.dump(best_rnn, _f)\n",
    "\n",
    "    return k_e_penul, k_e_rnn\n",
    "\n",
    "def knn_from_pkl(knn_path, y):\n",
    "    with open(knn_path, 'rb') as _f:\n",
    "        knn = pickle.load(_f)\n",
    "    \n",
    "    y_pca = knn['pca'].transform(y)\n",
    "    \n",
    "    pred_distances, pred_indices = knn['knn'].kneighbors(y_pca)\n",
    "\n",
    "    inverse_dist = pred_distances ** knn['exponent']\n",
    "\n",
    "    norm_inverse_dist = inverse_dist / inverse_dist.sum(axis = 1)[:, None]\n",
    "\n",
    "    weigted_value = torch.squeeze(knn['train_gt'][pred_indices]) * norm_inverse_dist\n",
    "    \n",
    "    prediction = weigted_value.sum(axis = 1).float()\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a83d0f-ef73-497f-a01f-22825bcdae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./pytorch_models/78304.pt\"\n",
    "model = knn_net(np.array(x_train).shape[-1])\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# LT10\n",
    "# LT 10 and LT50 have 1-1 matching GT\n",
    "train_gt_lt10, train_rnn, train_penul = get_sets(\n",
    "    x_train, y_train[:, :, 0], x_train, model)\n",
    "print(\"train_gt_lt10, train_rnn, train_penul\",\n",
    "      train_gt_lt10.shape, train_rnn.shape, train_penul.shape)\n",
    "\n",
    "test_gt_lt10, test_rnn, test_penul = get_sets(\n",
    "    x_test, y_test[:, :, 0], x_test, model)\n",
    "print(\"test_gt_lt10, test_rnn, test_penul\",\n",
    "      test_gt_lt10.shape, test_rnn.shape, test_penul.shape)\n",
    "\n",
    "# LT50\n",
    "# LT 10 and LT50 have 1-1 matching GT\n",
    "train_gt_lt50, _, _ = get_sets(x_train, y_train[:, :, 1], x_train, model)\n",
    "print(\"train_gt_lt50.shape\", train_gt_lt50.shape)\n",
    "\n",
    "test_gt_lt50, _, _ = get_sets(x_test, y_test[:, :, 1], x_test, model)\n",
    "print(\"test_gt_lt50.shape\", test_gt_lt50.shape)\n",
    "\n",
    "# BudBreak\n",
    "train_gt_bb, _, _ = get_sets(x_train, y_train[:, :, 3], x_train, model)\n",
    "print(\"train_gt_bb.shape\", train_gt_bb.shape)\n",
    "\n",
    "test_gt_bb, _, _ = get_sets(x_test, y_test[:, :, 3], x_test, model)\n",
    "print(\"test_gt_bb.shape\", test_gt_bb.shape)\n",
    "\n",
    "# LT10\n",
    "k_e_penul_lt10, k_e_rnn_lt10 = knn_param_search(train_penul=train_penul, test_penul=test_penul, train_rnn=train_rnn,\n",
    "                                                test_rnn=test_rnn, train_gt=train_gt_lt10, test_gt=test_gt_lt10, criterion=nn.MSELoss(), name=\"LT10\")\n",
    "print(\"k_e_penul_lt10\", k_e_penul_lt10)\n",
    "print(\"k_e_rnn_lt10\", k_e_rnn_lt10)\n",
    "\n",
    "# LT50\n",
    "k_e_penul_lt50, k_e_rnn_lt50 = knn_param_search(train_penul=train_penul, test_penul=test_penul, train_rnn=train_rnn,\n",
    "                                                test_rnn=test_rnn, train_gt=train_gt_lt50, test_gt=test_gt_lt50, criterion=nn.MSELoss(), name=\"LT50\")\n",
    "print(\"k_e_penul_lt50\", k_e_penul_lt50)\n",
    "print(\"k_e_rnn_lt50\", k_e_rnn_lt50)\n",
    "\n",
    "# LT90\n",
    "\n",
    "# LT90 Merlot has fewer GT\n",
    "train_gt_lt90, train_rnn_lt90, train_penul_lt90 = get_sets(\n",
    "    x_train, y_train[:, :, 2], x_train, model)\n",
    "print(\"train_gt_lt90.shape, train_rnn_lt90.shape, train_penul_lt90.shape\",\n",
    "      train_gt_lt90.shape, train_rnn_lt90.shape, train_penul_lt90.shape)\n",
    "\n",
    "test_gt_lt90, test_rnn_lt90, test_penul_lt90 = get_sets(\n",
    "    x_test, y_test[:, :, 2], x_test, model)\n",
    "print(\"test_gt_lt90.shape, test_rnn_lt90.shape, test_penul_lt90.shape\",\n",
    "      test_gt_lt90.shape, test_rnn_lt90.shape, test_penul_lt90.shape)\n",
    "\n",
    "k_e_penul_lt90, k_e_rnn_lt90 = knn_param_search(train_penul=train_penul_lt90, test_penul=test_penul_lt90, train_rnn=train_rnn_lt90,\n",
    "                                                test_rnn=test_rnn_lt90, train_gt=train_gt_lt90, test_gt=test_gt_lt90, criterion=nn.MSELoss(), name=\"LT90\")\n",
    "print(\"k_e_penul_lt90\", k_e_penul_lt90)\n",
    "print(\"k_e_rnn_lt90\", k_e_rnn_lt90)\n",
    "\n",
    "# BudBreak\n",
    "train_gt_bb, train_rnn_bb, train_penul_bb = get_sets(\n",
    "    x_train, y_train[:, :, 3], x_train, model)\n",
    "print(\"train_gt_bb.shape, train_rnn_bb.shape, train_penul_bb.shape\",\n",
    "      train_gt_bb.shape, train_rnn_bb.shape, train_penul_bb.shape)\n",
    "\n",
    "test_gt_bb, test_rnn_bb, test_penul_bb = get_sets(\n",
    "    x_test, y_test[:, :, 3], x_test, model)\n",
    "print(\"test_gt_bb.shape, test_rnn_bb.shape, test_penul_bb.shape\",\n",
    "      test_gt_bb.shape, test_rnn_bb.shape, test_penul_bb.shape)\n",
    "\n",
    "k_e_penul_bb, k_e_rnn_bb = knn_param_search(train_penul=train_penul_bb, test_penul=test_penul_bb, train_rnn=train_rnn_bb,\n",
    "                                            test_rnn=test_rnn_bb, train_gt=train_gt_bb, test_gt=test_gt_bb, criterion=nn.MSELoss(), name=\"Budbreak\")\n",
    "print(\"k_e_penul_bb\", k_e_penul_bb)\n",
    "print(\"k_e_rnn_bb\", k_e_rnn_bb)\n",
    "\n",
    "print(\"FINAL\")\n",
    "print(\"clusters / exponent / PCA / n components\")\n",
    "print(\"k_e_penul_lt10 =\", k_e_penul_lt10)\n",
    "print(\"k_e_rnn_lt10 =\", k_e_rnn_lt10)\n",
    "print(\"k_e_penul_lt50 =\", k_e_penul_lt50)\n",
    "print(\"k_e_rnn_lt50 =\", k_e_rnn_lt50)\n",
    "print(\"k_e_penul_lt90 =\", k_e_penul_lt90)\n",
    "print(\"k_e_rnn_lt90 =\", k_e_rnn_lt90)\n",
    "print(\"k_e_penul_bb =\", k_e_penul_bb)\n",
    "print(\"k_e_rnn_bb =\", k_e_rnn_bb)\n",
    "_, test_rnn_bb_all, _ = get_sets(x_test, y_test[:, :, 3], x_test, model, test_set=True)\n",
    "pred_bb = knn_from_pkl(budbreak_knn_path, test_rnn_bb_all)\n",
    "print(pred_bb.shape)\n",
    "results_2020_bb = pred_bb[:252].numpy()\n",
    "results_2021_bb = pred_bb[252:].numpy()\n",
    "\n",
    "print(results_2020_bb.shape)\n",
    "print(results_2021_bb.shape)\n",
    "plot_budbreak(\"KNN Budbreak 2021-2022\", seasons[-2], results_2020_bb)\n",
    "plot_budbreak(\"KNN Budbreak 2022-*\", seasons[-1], results_2021_bb)\n",
    "gt = y_test.cpu().detach().numpy()\n",
    "gt_2020_lt10 = gt[0, :, 0]\n",
    "gt_2021_lt10 = gt[1, :, 0]\n",
    "\n",
    "gt_2020_lt50 = gt[0, :, 1]\n",
    "gt_2021_lt50 = gt[1, :, 1]\n",
    "\n",
    "gt_2020_lt90 = gt[0, :, 2]\n",
    "gt_2021_lt90 = gt[1, :, 2]\n",
    "\n",
    "print(\"gt 2020 LT10\", gt_2020_lt10.shape)\n",
    "print(\"gt 2021 LT10\", gt_2021_lt10.shape)\n",
    "\n",
    "print(\"gt 2020 LT50\", gt_2020_lt50.shape)\n",
    "print(\"gt 2021 LT50\", gt_2021_lt50.shape)\n",
    "\n",
    "print(\"gt 2020 LT90\", gt_2020_lt90.shape)\n",
    "print(\"gt 2021 LT90\", gt_2021_lt90.shape)\n",
    "\n",
    "ferg_model = []\n",
    "\n",
    "for i, season in enumerate(seasons):\n",
    "    if i == 23 or i == 24: # last 2 seasons for testing\n",
    "        _y = df[[\"PREDICTED_Hc\"]].loc[season, :].to_numpy()\n",
    "        \n",
    "        add_array = np.zeros((season_max_length - len(season), 1))\n",
    "        add_array[:] = np.NaN\n",
    "\n",
    "        _y = np.concatenate((_y, add_array), axis=0)\n",
    "        \n",
    "        ferg_model.append(_y)\n",
    "\n",
    "print(\"ferg 2020\", ferg_model[0].shape)\n",
    "print(\"ferg 2021\", ferg_model[1].shape)\n",
    "_, test_rnn_lt10_all, _ = get_sets(x_test, y_test[:, :, 0], x_test, model, test_set=True)\n",
    "pred_lt10 = knn_from_pkl(lt10_knn_path, test_rnn_lt10_all)\n",
    "print(pred_lt10.shape)\n",
    "results_2020_lt10 = pred_lt10[:252].numpy()\n",
    "results_2021_lt10 = pred_lt10[252:].numpy()\n",
    "\n",
    "print(results_2020_lt10.shape)\n",
    "print(results_2020_lt10.shape)\n",
    "graph_residual_tempdiff(\"KNN LT10 Same Day | 2020-2021 Season |  GT - Pred\", gt_2020_lt10 - results_2020_lt10)\n",
    "graph_residual_tempdiff(\"KNN LT10 Same Day | 2021-* Season |  GT - Pred\", gt_2021_lt10 - results_2020_lt10)\n",
    "_, test_rnn_lt50_all, _ = get_sets(x_test, y_test[:, :, 1], x_test, model, test_set=True)\n",
    "pred_lt50 = knn_from_pkl(lt50_knn_path, test_rnn_lt50_all)\n",
    "print(pred_lt10.shape)\n",
    "results_2020_lt50 = pred_lt50[:252].numpy()\n",
    "results_2021_lt50 = pred_lt50[252:].numpy()\n",
    "\n",
    "print(results_2020_lt50.shape)\n",
    "print(results_2021_lt50.shape)\n",
    "graph_residual_ferg(\"KNN LT50 Same Day | 2020-2021 Season |  GT - Pred\", gt_2020_lt50 - ferg_model[0].flatten(), gt_2020_lt50 - results_2020_lt50)\n",
    "graph_residual_ferg(\"KNN LT50 Same Day | 2021-* Season |  GT - Pred\", gt_2021_lt50 - ferg_model[1].flatten(), gt_2021_lt50 - results_2021_lt50)\n",
    "_, test_rnn_lt90_all, _ = get_sets(x_test, y_test[:, :, 2], x_test, model, test_set=True)\n",
    "pred_lt90 = knn_from_pkl(lt90_knn_path, test_rnn_lt90_all)\n",
    "\n",
    "print(pred_lt90.shape)\n",
    "results_2020_lt90 = pred_lt90[:252].numpy()\n",
    "\n",
    "results_2021_lt90 = pred_lt90[252:].numpy()\n",
    "\n",
    "print(results_2020_lt90.shape)\n",
    "print(results_2020_lt90.shape)\n",
    "graph_residual_tempdiff(\"KNN lt90 Same Day | 2020-2021 Season |  GT - Pred\", gt_2020_lt90 - results_2020_lt90)\n",
    "graph_residual_tempdiff(\"KNN lt90 Same Day | 2021-* Season |  GT - Pred\", gt_2021_lt90 - results_2020_lt90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
