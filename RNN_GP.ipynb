{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3674d0ee-0d57-4386-8cfd-b86c2448e866",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99b7ad-24b1-423c-8fd3-e6d52c20ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats.mstats import winsorize\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d6d8c-9a25-44b6-97fb-234987bfa841",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0ae56-8d83-4a7f-bae2-bc290ca7f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path = \"./data\"\n",
    "output_path = \"./consolidated_data\"\n",
    "csv_paths = []\n",
    "data_file_dict = {}\n",
    "idx = 0\n",
    "for f in os.walk(all_data_path):\n",
    "    rootFolder = f[0]\n",
    "    for fname in f[-1]:\n",
    "        if (\".csv\" in fname):\n",
    "            csv_paths.append(os.path.join(rootFolder, fname))\n",
    "            data_file_dict[fname] = idx\n",
    "            idx += 1\n",
    "df = pd.read_csv(csv_paths[data_file_dict['ColdHardiness_Grape_Merlot.csv']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ceddbb-f590-4ebd-8781-718d996180f2",
   "metadata": {},
   "source": [
    "### defining features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bcd12-06a9-4a92-96b7-d9f89712f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_length = 30 # days\n",
    "# exclude SEASON_JDAY\n",
    "features = [#'SEASON_JDAY',\n",
    "       'PREDICTED_Hc', 'MEAN_AT', 'MIN_AT', 'AVG_AT', 'MAX_AT',\n",
    "       'MIN_REL_HUMIDITY', 'AVG_REL_HUMIDITY', 'MAX_REL_HUMIDITY', 'MIN_DEWPT',\n",
    "       'AVG_DEWPT', 'MAX_DEWPT', 'P_INCHES', 'WS_MPH', 'MAX_WS_MPH',\n",
    "       'LW_UNITY', 'ETO',\n",
    "       'ETR']\n",
    "# PREDICTED_Hc, MEAN_AT, MIN_AT, MAX_AT, MIN_REL_HUMIDITY, AVG_REL_HUMIDITY, MAX_REL_HUMIDITY, MIN_DEWPT, AVG_DEWPT, MAX_DEWPT, P_INCHES,WS_MPH, MAX_WS_MPH\n",
    "# label = ['LT10', 'LT50', 'LT90']\n",
    "label = ['LT50']\n",
    "print(len(features))\n",
    "# # get all indexes where LT10 is not null\n",
    "# idx_LT10_not_null = df[df['LT10'].notnull()].index.tolist()\n",
    "idx_LT50_not_null = df[df['LT50'].notnull()].index.tolist()\n",
    "# idx_LT90_not_null = df[df['LT90'].notnull()].index.tolist()\n",
    "\n",
    "# idx_LT_not_null = set(idx_LT10_not_null) & set(idx_LT50_not_null) & set(idx_LT90_not_null) # intersection where LT10, LT50, LT90 are not null\n",
    "idx_LT_not_null = idx_LT50_not_null\n",
    "print(len(idx_LT_not_null))\n",
    "dormant_seasons = df[df[\"DORMANT_SEASON\"] == 1]\n",
    "display(dormant_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02bbf2-b3e7-4847-a23b-f7b13c98f0d0",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7aa8e0-9f90-4046-b8b8-dfbe4f1b1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of timeseries examples\n",
    "# timeseries_idx = []\n",
    "\n",
    "# for _, idx in enumerate(idx_LT_not_null):\n",
    "#     if (idx - timeseries_length) > 0:\n",
    "#         timeseries_idx.append(np.arange(idx - timeseries_length + 1, idx + 1))\n",
    "\n",
    "# timeseries_idx = np.array(timeseries_idx)\n",
    "\n",
    "# print(idx_LT_not_null)\n",
    "# print(\"---\")\n",
    "# print(timeseries_idx.shape)\n",
    "# print(timeseries_idx[:5])\n",
    "# # display(df.loc[timeseries_idx[0].tolist()])\n",
    "\n",
    "seasons = []\n",
    "last_x = 0\n",
    "idx = -1\n",
    "season_max_length = 0\n",
    "for x in df[df[\"DORMANT_SEASON\"] == 1].index.tolist():\n",
    "    if x - last_x > 1:\n",
    "        seasons.append([])\n",
    "        if idx > -1:\n",
    "            season_max_length = max(season_max_length, len(seasons[idx]))\n",
    "        idx += 1\n",
    "    seasons[idx].append(x)\n",
    "    last_x = x\n",
    "season_max_length = max(season_max_length, len(seasons[idx]))\n",
    "print(len(seasons))\n",
    "print([len(x) for x in seasons])\n",
    "print(season_max_length)\n",
    "print(\"max:\")\n",
    "print(df[features].max())\n",
    "print(\"min:\")\n",
    "print(df[features].min())\n",
    "print(\"median:\")\n",
    "print(df[features].median())\n",
    "\n",
    "print(\"number of missing temperatures\")\n",
    "print((df == -100).sum())\n",
    "df[features].hist(figsize = (60, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a41d06-352b-4e03-940e-e82683e82a6a",
   "metadata": {},
   "source": [
    "### Linear Interpolation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b82d14-8e84-42bf-88b9-902031470ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_impute(number, last, next):\n",
    "    return [ last + (next - last) / (number + 1) * (n + 1) for n in range(number)]\n",
    "def data_prepocess(x, is_one_hot = True, is_normal = True):\n",
    "    # 4. YEAR_JDAY needs to be one-hot encoded\n",
    "    def one_hot_encoded(data, idx, one_hot_size, divided_by):\n",
    "        data = np.array(data)\n",
    "        data = np.nan_to_num(data)\n",
    "        batch, r_size, f_size = data.shape\n",
    "        return_data = np.zeros((batch, r_size, f_size + one_hot_size - 1))\n",
    "        for i, _x in enumerate(data):\n",
    "\n",
    "            oh_feature = _x[:, idx]\n",
    "\n",
    "            one_hot_v = np.zeros((oh_feature.shape[idx], one_hot_size))\n",
    "            one_hot_v[np.arange(oh_feature.size), np.floor(oh_feature / divided_by).astype(int)] = 1\n",
    "            return_data[i] = np.concatenate((_x[:, 1:], one_hot_v), axis = 1)\n",
    "\n",
    "        return return_data\n",
    "        # idx of YEAR_JDAY is 0\n",
    "    if is_one_hot:\n",
    "        x_m = one_hot_encoded(x, 0, 12, 31)\n",
    "    else:\n",
    "        x_m = np.nan_to_num(np.array(x))\n",
    "        \n",
    "    # 5. Data needs to be normalized.\n",
    "    if not is_one_hot:\n",
    "        NORMALIE_FEATURES = features\n",
    "    else:\n",
    "        NORMALIE_FEATURES = [#'SEASON_JDAY',\n",
    "       'PREDICTED_Hc', 'MEAN_AT', 'MIN_AT', 'MAX_AT',\n",
    "       'MIN_REL_HUMIDITY', 'AVG_REL_HUMIDITY', 'MAX_REL_HUMIDITY', 'MIN_DEWPT',\n",
    "       'AVG_DEWPT', 'MAX_DEWPT', 'P_INCHES', 'WS_MPH', 'MAX_WS_MPH']#, \"diff_Hc\"]\n",
    "    NORMALIE_FEATURES_idx = np.array(range(0, len(NORMALIE_FEATURES)))\n",
    "    if is_normal:\n",
    "        x_mean = df[NORMALIE_FEATURES].mean().to_numpy()\n",
    "        x_std = df[NORMALIE_FEATURES].std().to_numpy()\n",
    "        x_min = df[NORMALIE_FEATURES].min().to_numpy()\n",
    "        x_max = df[NORMALIE_FEATURES].max().to_numpy()\n",
    "        print(x_mean)\n",
    "        print(x_std)\n",
    "        # print(x_m[0, 0:4, :])\n",
    "        x_m[:, :, NORMALIE_FEATURES_idx] = (x_m[:, :, NORMALIE_FEATURES_idx] - x_mean) / x_std\n",
    "        # x_m[:, :, NORMALIE_FEATURES_idx] = (x_m[:, :, NORMALIE_FEATURES_idx] - x_min) / (x_max - x_min)\n",
    "        # print(x_m[0, 0:4, :])\n",
    "        # print(x_m.shape)\n",
    "    \n",
    "    return x_m\n",
    "\n",
    "last_value = None\n",
    "miss_flag = False\n",
    "miss_idx = []\n",
    "lable_with_miss_value = df.loc[: , (df == -100).any()].columns\n",
    "for lb in lable_with_miss_value:\n",
    "    for i, value in enumerate(df[lb]):\n",
    "        if value == -100:\n",
    "            if not miss_flag:\n",
    "                last_value = df[lb][i - 1]\n",
    "                miss_flag = True\n",
    "            miss_idx.append(i)\n",
    "        else:\n",
    "            if miss_flag:\n",
    "                fill_values = linear_impute(len(miss_idx), last_value, value)\n",
    "                for j, mi in enumerate(miss_idx):\n",
    "                    df.loc[mi,lb] = fill_values[j]\n",
    "                miss_flag = False\n",
    "                miss_idx = []\n",
    "                last_value = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176138a-2139-43cc-9f15-91aa1b9a4aa0",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b05bc6-33f7-4b28-a2d6-cb39fafb7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. imputate the missing Predicted_Hc  \n",
    "\n",
    "lb = \"PREDICTED_Hc\"\n",
    "miss_flag = False\n",
    "start_flag = False\n",
    "miss_idx = []\n",
    "for i, value in enumerate(df[lb]):\n",
    "    if pd.isnull(value) and not start_flag:\n",
    "        continue\n",
    "    else:\n",
    "        start_flag = True\n",
    "    if pd.isnull(value):\n",
    "        if not miss_flag:\n",
    "            last_value = df[lb][i - 1]\n",
    "            miss_flag = True\n",
    "        miss_idx.append(i)\n",
    "    else:\n",
    "        if miss_flag:\n",
    "            fill_values = linear_impute(len(miss_idx), last_value, value)\n",
    "            for j, mi in enumerate(miss_idx):\n",
    "                df.loc[mi,lb] = fill_values[j]\n",
    "            miss_flag = False\n",
    "            miss_idx = []\n",
    "            last_value = None\n",
    "            \n",
    "# other data imputation\n",
    "print((pd.isnull(df)).sum())\n",
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors = 100)\n",
    "# df[features] = imputer.fit_transform(df[features])\n",
    "# print((pd.isnull(df)).sum())\n",
    "\n",
    "df[features].hist(figsize = (60, 50))\n",
    "\n",
    "\n",
    "# replace PREDICTED_Hc with difference of two days.\n",
    "# values = []\n",
    "# last_value = None\n",
    "# for i, value in enumerate(df[\"PREDICTED_Hc\"]):\n",
    "#     if not math.isnan(value):\n",
    "#         # print(value)\n",
    "#         if last_value is not None:\n",
    "#             values.append(value - last_value)\n",
    "#         else:\n",
    "#             values.append(0)\n",
    "#         last_value = value\n",
    "#     else:\n",
    "#         values.append(value)\n",
    "#         last_value = None \n",
    "# # print(values)\n",
    "# df[\"diff_Hc\"] = values\n",
    "# features.append(\"diff_Hc\")\n",
    "# print(\"min:\")\n",
    "# print(df[features].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950eb7a-fe54-4422-a621-df550acf0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x = []\n",
    "original_y = []\n",
    "for i, season in enumerate(seasons):\n",
    "    _x = (df[features].loc[season, :]).to_numpy()\n",
    "    # print(_x.shape)\n",
    "    _x = np.concatenate((_x, np.zeros((season_max_length - len(season), len(features)))), axis = 0)\n",
    "    # print(season)\n",
    "    # print(_x.shape)\n",
    "    \n",
    "    add_array = np.zeros(season_max_length - len(season))\n",
    "    add_array[:] = np.NaN\n",
    "    # print(len(season),add_array[1:].shape)\n",
    "    _y_same_day = df.loc[season,:][['LT50']].to_numpy() - df.loc[season,:][['PREDICTED_Hc']].to_numpy()\n",
    "    _y_same_day = np.concatenate((_y_same_day.flatten(), add_array), axis = 0)[:, None]\n",
    "    _y_next_day = np.concatenate((np.array([np.NaN]), _y_same_day.flatten()[:-1]), axis = 0)[:, None]\n",
    "    # print(_y_same_day.shape, _y_next_day.shape)\n",
    "    _y = np.concatenate((_y_same_day, _y_next_day), axis = 1)\n",
    "          \n",
    "    # df_s = df.loc[season,:]\n",
    "    # _y = np.zeros(season_max_length)\n",
    "    # _y[_y == 0] = np.NaN\n",
    "    # season_not_null_idx = df_s[df_s['LT50'].notnull()].index\n",
    "    # _y[(season_not_null_idx - season[0]).to_numpy()] = df.loc[season_not_null_idx, :]['LT50'].to_numpy()# - df.loc[season_not_null_idx, :]['PREDICTED_Hc'].to_numpy()\n",
    "    # # print(y_gt)\n",
    "    # # _y = ([(season_not_null_idx - season[0]).tolist(), y_gt.tolist()])\n",
    "    # # print(_x)\n",
    "    # print(len(_y), _y)\n",
    "    # input()\n",
    "    original_x.append(_x)\n",
    "    original_y.append(_y)\n",
    "original_x = np.array(original_x)\n",
    "original_y = np.array(original_y)\n",
    "# data pre-processed\n",
    "print(np.array(original_x).shape)\n",
    "print(np.array(original_y).shape)\n",
    "\n",
    "# print(np.isnan(original_x).sum(axis = 1))\n",
    "\n",
    "    \n",
    "x_all = data_prepocess(original_x, is_one_hot = False, is_normal = True)\n",
    "y_all = np.array(original_y)\n",
    "print(x_all.shape)\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc0b07-8bef-438e-98e0-fe884ee948bb",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40909b-9ec2-48eb-a831-fb80a2b336a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(net, self).__init__()\n",
    "        self.num_out = len(label)\n",
    "        self.numLayers = 1\n",
    "        self.memory_size = 1024\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, 256)\n",
    "        self.linear_extra = nn.Linear(256, 512)\n",
    "        self.rnn = nn.GRU(input_size=512, hidden_size=self.memory_size, num_layers=self.numLayers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.linear_s_1 = nn.Linear(self.memory_size, 512)\n",
    "        self.linear_s_2 = nn.Linear(512, self.num_out)\n",
    "        \n",
    "        self.linear_n_1 = nn.Linear(self.memory_size, 512)\n",
    "        self.linear_n_2 = nn.Linear(512, self.num_out)\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        batch_dim, time_dim, state_dim = x.shape\n",
    "        # print(batch_dim, time_dim, state_dim)\n",
    "        out = self.linear1(x).relu()\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear_extra(out).relu()\n",
    "            \n",
    "        if h is None:\n",
    "            h = torch.zeros(self.numLayers, batch_dim, self.memory_size, device=x.device)\n",
    "            # c = torch.zeros(self.numLayers, batch_dim, 1024, device=x.device)\n",
    "        # print(out.shape, h.shape)\n",
    "        # out, h_next = self.rnn(out, (h, c))\n",
    "        out, h_next = self.rnn(out, h)\n",
    "        # out = out[:, -1, :].relu()\n",
    "        \n",
    "        out_s = self.dropout(out)\n",
    "        out_s = self.linear_s_1(out_s).relu()\n",
    "        out_s_last = out_s.detach().clone()\n",
    "        out_s = self.dropout(out_s)\n",
    "        out_s = self.linear_s_2(out_s)\n",
    "\n",
    "        # out_n = self.dropout(out)\n",
    "        # out_n = self.linear_s_1(out_n).relu()\n",
    "        # out_n = self.dropout(out_n)\n",
    "        # out_n = self.linear_s_2(out_n)\n",
    "        # return out_s, out_n, h_next\n",
    "        return out_s, out.detach(), out_s_last.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55382b48-7372-4da4-913a-65ae73b692c0",
   "metadata": {},
   "source": [
    "### Set up training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94575c7b-2f27-45d7-b5a2-36438530a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfEpochs = 1#500\n",
    "batchSize = 12\n",
    "random_seed = 0\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.manual_seed(random_seed)\n",
    "save_name = \"best_residual_rnn_gps.pt\"\n",
    "model = net(np.array(x_all).shape[-1])\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay = 0.08)\n",
    "criterion = nn.MSELoss()\n",
    "criterion.to(device)\n",
    "log_dir = \"./logs/\"\n",
    "log_train = \"frost-mitigation/train_sday_gps\"\n",
    "log_val = \"frost-mitigation/val_sday_whole_gps\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0ec45-56de-4780-b9bf-1a2d99e26c76",
   "metadata": {},
   "source": [
    "### Set up torch dataset input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2156717-a92c-48d5-8bac-8a68a692db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n",
    "indices = np.arange(x_all.shape[0])\n",
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(x_all, y_all, indices, test_size=0.07, shuffle = False)#, random_state = random_seed)\n",
    "print(indices_train, indices_test)\n",
    "print(x_all.shape)\n",
    "print(y_all.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(y_train[0])\n",
    "print(y_test.shape)\n",
    "# Be careful of replacing nan with zeros\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "trainLoader = DataLoader(train_dataset, batch_size=batchSize)\n",
    "\n",
    "val_dataset = TensorDataset(x_test, y_test)\n",
    "valLoader = DataLoader(val_dataset, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37262a3b-b58a-4ffa-a327-74f832c2c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "min_test_loss = float(\"inf\")\n",
    "def get_valuable_idx(y):\n",
    "    return np.argwhere(np.isnan(y) == False) \n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), './saved_model/{}'.format(name))\n",
    "def load_model(model, name):\n",
    "    model.load_state_dict(torch.load('./saved_model/{}'.format(name)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee11503-4178-43b6-80e3-ae61c122bcd2",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf4534-b2f2-4f17-96b7-b611ec296ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "np.set_printoptions(threshold=10000)\n",
    "for epoch in range(numberOfEpochs):\n",
    "  \n",
    "  # Training Loop\n",
    "    with tqdm(trainLoader, unit=\"batch\") as tepoch: \n",
    "        model.train()\n",
    "\n",
    "        tepoch.set_description(f\"Epoch {epoch + 1}/{numberOfEpochs} [T]\")\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for i, (x, y) in enumerate(trainLoader):\n",
    "            count += 1\n",
    "            x_torch = x.to(device)\n",
    "            y_torch = y.to(device)\n",
    "            output_s, _, _ = model(x_torch) \n",
    "            # output_s, output_n, _ = model(x) \n",
    "            optimizer.zero_grad()       # zero the parameter gradients\n",
    "            valuable_idx = get_valuable_idx(y[:, :, 0])\n",
    "            # print(valuable_idx)\n",
    "            # print(output_s.shape)\n",
    "            # print(y.shape)\n",
    "            loss_s = criterion(output_s[valuable_idx[0], valuable_idx[1], :], y_torch[:, :, 0][valuable_idx[0], valuable_idx[1]][:,None])\n",
    "            # print(output_s[valuable_idx[0], valuable_idx[1]].view(-1).shape)\n",
    "            # print(y[:, :, 0][valuable_idx[0], valuable_idx[1]].shape)           \n",
    "            # print(output_s[valuable_idx[0], valuable_idx[1]].shape)\n",
    "            # input()\n",
    "            # valuable_idx = get_valuable_idx(y[:, :, 1])\n",
    "            # loss_n = criterion(output_n[valuable_idx[0], valuable_idx[1]], y[:, :, 1][valuable_idx[0], valuable_idx[1]][:,None])\n",
    "            \n",
    "            loss = loss_s# + loss_n\n",
    "            loss.backward()             # backward + \n",
    "            optimizer.step()            # optimize\n",
    "            total_loss += loss.item()\n",
    "            # tepoch.set_postfix(Train_Loss=loss.item())\n",
    "            tepoch.set_postfix(Train_Loss=total_loss / count)\n",
    "            tepoch.update(1)\n",
    "\n",
    "        writer.add_scalar(log_train, total_loss / count, epoch)\n",
    "        \n",
    "\n",
    "  # Validation Loop\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valLoader, unit=\"batch\") as tepoch: \n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}/{numberOfEpochs} [V]\")\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for i, (x, y) in enumerate(valLoader):\n",
    "                count += 1\n",
    "                x_torch = x.to(device)\n",
    "                y_torch = y.to(device)\n",
    "                pred_s, _, _ = model(x_torch)\n",
    "                # output_s, output_n, _ = model(x) \n",
    "                valuable_idx = get_valuable_idx(y[:, :, 0])\n",
    "                loss_s = criterion(pred_s[valuable_idx[0], valuable_idx[1]], y_torch[:, :, 0][valuable_idx[0], valuable_idx[1]][:,None])\n",
    "\n",
    "                # valuable_idx = get_valuable_idx(y[:, :, 1])\n",
    "                # loss_n = criterion(output_n[valuable_idx[0], valuable_idx[1]], y[:, :, 1][valuable_idx[0], valuable_idx[1]][:,None])\n",
    "\n",
    "                # total_loss += (loss_s.item() + loss_n.item()) / 2\n",
    "                total_loss += loss_s.item()\n",
    "                tepoch.set_postfix(Val_Loss=total_loss / count)\n",
    "                tepoch.update(1)\n",
    "            avg_loss = total_loss / count\n",
    "            writer.add_scalar(log_val, avg_loss, epoch)\n",
    "            if min_test_loss > avg_loss:\n",
    "                min_test_loss = avg_loss\n",
    "                best_model = deepcopy(model)\n",
    "                save_model(best_model, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60b2ce-df0a-45bc-b668-87925aa83955",
   "metadata": {},
   "source": [
    "### Data Preprocessing for GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf42bc-92bb-4267-a360-a5913ccc094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_um_set(x, y, ox, model, is_train = True):\n",
    "    with torch.no_grad():\n",
    "        PREDICTED_HC_IDX = 0\n",
    "        model.eval()\n",
    "        pred_s, pred_rnn, pred_last = model(x.to(device))\n",
    "        pred_s, pred_rnn, pred_last = pred_s.cpu(), pred_rnn.cpu(), pred_last.cpu()\n",
    "        y = y[:, :, 0]\n",
    "        if is_train:\n",
    "            valuable_idx = get_valuable_idx(y)\n",
    "            gt, pred_s, pred_rnn, pred_last = y[valuable_idx[0], valuable_idx[1]], \\\n",
    "                                              pred_s[valuable_idx[0], valuable_idx[1]], \\\n",
    "                                              pred_rnn[valuable_idx[0], valuable_idx[1]], \\\n",
    "                                              pred_last[valuable_idx[0], valuable_idx[1]]\n",
    "            predicted_hc = ox[valuable_idx[0], valuable_idx[1], PREDICTED_HC_IDX]\n",
    "        else:\n",
    "            valuable_idx = get_valuable_idx(y)\n",
    "            gt, pred_s, pred_rnn, pred_last = y.view(-1), \\\n",
    "                                              pred_s.reshape(pred_s.size(0) * pred_s.size(1), pred_s.size(2)), \\\n",
    "                                              pred_rnn.reshape(pred_rnn.size(0) * pred_rnn.size(1), pred_rnn.size(2)), \\\n",
    "                                              pred_last.reshape(pred_last.size(0) * pred_last.size(1), pred_last.size(2))\n",
    "            predicted_hc = ox[:, :, PREDICTED_HC_IDX].flatten()\n",
    "        LT50_value_pred = torch.tensor(predicted_hc[:, None]) + pred_s\n",
    "        LT50_value_gt = torch.tensor(predicted_hc) + gt\n",
    "        # print(LT50_value.shape, pred_rnn.shape, pred_last.shape)\n",
    "        # pred_rnn = torch.cat((pred_rnn.double(), LT50_value), dim = 1)\n",
    "        # pred_last = torch.cat((pred_last.double(), LT50_value), dim = 1)\n",
    "        # predicted_hc\n",
    "        # print(pred_s.shape, predicted_hc.shape, pred_rnn.shape, pred_last.shape)\n",
    "        # print(pred_rnn.shape, pred_last.shape)\n",
    "        \n",
    "        return gt, pred_s, predicted_hc, pred_rnn, pred_last\n",
    "\n",
    "PAC_THRESHOLD = 75\n",
    "def pca_rdc(train_set, test_set, pac_thres = PAC_THRESHOLD, n_components = None):\n",
    "    from sklearn.decomposition import PCA\n",
    "    if n_components is None:\n",
    "        pca = PCA()\n",
    "        pca.fit(train_set)\n",
    "        idx_reduction = pca.explained_variance_ratio_ >= np.percentile(pca.explained_variance_ratio_, pac_thres)\n",
    "        n_components = idx_reduction.sum()\n",
    "    \n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(train_set)\n",
    "\n",
    "    return pca.transform(train_set).astype(float), pca.transform(test_set).astype(float), n_components    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cfc2e-9ea0-4a7b-890c-2a918f36f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LT50_gt, train_LT50_pred, train_furguson, train_x_um_rnn, train_x_um_last = get_um_set(x_train, y_train, original_x[indices_train], best_model)\n",
    "print(train_LT50_gt.shape, train_LT50_pred.shape, train_furguson.shape, train_x_um_rnn.shape, train_x_um_last.shape)\n",
    "test_LT50_gt, test_LT50_pred, test_furguson, test_x_um_rnn, test_x_um_last = get_um_set(x_test, y_test, original_x[indices_test], best_model, is_train = False)\n",
    "print(test_LT50_gt.shape, test_LT50_pred.shape, test_furguson.shape, test_x_um_rnn.shape, test_x_um_last.shape, y_test.shape)\\\n",
    "\n",
    "# torch.Size([812, 1024]) torch.Size([812]) torch.Size([756, 1024]) torch.Size([3, 252]) torch.Size([756])\n",
    "train_x_um_rnn_rdc, test_x_um_rnn_rdc, n_components = pca_rdc(train_x_um_rnn, test_x_um_rnn, n_components = 3)\n",
    "print(train_x_um_rnn.shape, test_x_um_rnn.shape)\n",
    "valuable_idx = get_valuable_idx(y_test[:, :, 0])\n",
    "truth = y_test[:,:, 0][valuable_idx[0],valuable_idx[1]][:, None]\n",
    "\n",
    "baseline = torch.tensor([train_LT50_gt.mean()] * truth.shape[0])\n",
    "# print(baseline)\n",
    "# print(truth)\n",
    "print(criterion(baseline, truth[:,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0d70a-a993-4bd0-8c3e-0c23f06e6d28",
   "metadata": {},
   "source": [
    "### GP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5c7b7-e739-4b55-bdac-19cbc06a736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be6990-22b8-4c21-85cb-f8a96e81b755",
   "metadata": {},
   "source": [
    "### GP Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830d5f9-9674-494f-a7ff-979dbb799e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "\n",
    "def IDW_GPS(train_x, train_y, test_x, test_y, test_LT50_y, training_iter = 2000, lr=0.01, verbose = False):\n",
    "    train_x, train_y, test_x, test_y, test_LT50_y = \\\n",
    "    torch.tensor(train_x), torch.tensor(train_y), torch.tensor(test_x), torch.tensor(test_y), torch.tensor(test_LT50_y)\n",
    "    train_x_gpu, train_y_gpu, test_x_gpu, test_y_gpu = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n",
    "    print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, test_LT50_y.shape)\n",
    "    print(type(train_x), type(train_y), type(test_x), type(test_y), type(test_LT50_y))\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "    model.to(device)\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "    valuable_idx = get_valuable_idx(test_y)\n",
    "    # print(test_y)\n",
    "    # print(valuable_idx)\n",
    "    val_idx = valuable_idx[1][valuable_idx[0] == 0]\n",
    "    # print(valuable_idx)\n",
    "    for i in range(1, test_y.shape[0]):\n",
    "        val_idx = torch.cat((val_idx, valuable_idx[1][valuable_idx[0] == i] + test_y.shape[1] * i))\n",
    "    # print(val_idx)\n",
    "    valid_y = test_y[valuable_idx[0], valuable_idx[1]]\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    mll.to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_pred = None\n",
    "    for i in tqdm(range(training_iter)):\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x_gpu)\n",
    "        # Calc loss and backprop gradients\n",
    "        # print(train_y.shape)\n",
    "        loss = -mll(output, train_y_gpu)\n",
    "        loss.backward()\n",
    "        \n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        f_preds = model(test_x_gpu)\n",
    "        y_preds = likelihood(model(test_x_gpu))\n",
    "\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        f_covar = f_preds.covariance_matrix\n",
    "\n",
    "        y_mean = y_preds.mean\n",
    "        y_var = y_preds.variance\n",
    "        y_covar = y_preds.covariance_matrix\n",
    "\n",
    "        # print(f_mean[val_idx].shape, valid_y.shape)\n",
    "        val_loss = criterion(y_mean[val_idx], valid_y.to(device))\n",
    "        # print(y_mean[val_idx].shape, valid_y.shape)\n",
    "        if verbose and i % 100 == 0:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f   Val Loss: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.item(),\n",
    "                model.likelihood.noise.item(),\n",
    "                val_loss,\n",
    "            ))\n",
    "        if best_loss >= val_loss:\n",
    "            best_model = model\n",
    "            best_loss = val_loss\n",
    "            best_pred = y_preds\n",
    "        optimizer.step()\n",
    "    return best_pred, val_idx, best_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa5dd4-fabd-40be-b87c-147a90df3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_best_loss, val_last_idx, gp_best_pred, gp_best_model = IDW_GPS(train_x_um_rnn, train_LT50_gt, test_x_um_rnn, y_test[:, :, 0], test_LT50_gt, training_iter = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11e021-4a72-47e9-84eb-34871f8fef79",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5edf6-2078-4e44-8a29-7850d733c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _graph_bar(graph_name, prediction, ground_truth):\n",
    "    print(\"p\", prediction[:5])\n",
    "    print(\"gt\", ground_truth[:5])\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    ind = np.arange(prediction.shape[0])\n",
    "\n",
    "    width = 0.3\n",
    "\n",
    "    plt.bar(ind, prediction , width, label='Prediction')\n",
    "    plt.bar(ind + width, ground_truth, width, label='GT')\n",
    "\n",
    "    plt.xlabel('Example Index')\n",
    "    plt.ylabel('Temperature. (C)')\n",
    "    plt.title(graph_name)\n",
    "\n",
    "    plt.legend(loc='best', prop={'size': 20})\n",
    "    plt.savefig('bargraph.png')\n",
    "    #plt.show()\n",
    "    return\n",
    "\n",
    "def _graph_bar_diff(graph_name, prediction, ground_truth):\n",
    "    prediction = np.absolute(prediction)\n",
    "    ground_truth = np.absolute(ground_truth)\n",
    "\n",
    "    diff = ground_truth - prediction\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    ind = np.arange(prediction.shape[0])\n",
    "\n",
    "    width = 0.3\n",
    "\n",
    "    plt.bar(ind, diff, width, label='GT - Pred')\n",
    "\n",
    "    plt.xlabel('Example Index')\n",
    "    plt.ylabel('Temperature. (C)')\n",
    "    plt.title(graph_name)\n",
    "\n",
    "    plt.legend(loc='best', prop={'size': 20})\n",
    "    plt.savefig('diffbargraph.png')\n",
    "    #plt.show()\n",
    "    return\n",
    "\n",
    "def _graph_scatter(graph_name, ground_truth, prediction):\n",
    "    prediction = np.absolute(prediction)\n",
    "    print(np.amin(prediction), np.amax(prediction))\n",
    "\n",
    "    ground_truth = np.absolute(ground_truth)\n",
    "    print(np.amin(ground_truth), np.amax(ground_truth))\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    plt.title(graph_name)\n",
    "\n",
    "    plt.scatter(x=ground_truth, y=prediction)\n",
    "\n",
    "    default_x_ticks = range(ground_truth.shape[0])\n",
    "\n",
    "    plt.xticks(default_x_ticks, ground_truth)\n",
    "\n",
    "    plt.xlabel('ground_truth (C)')\n",
    "\n",
    "    plt.ylabel('prediction (C)')\n",
    "\n",
    "    plt.axis('square')\n",
    "    plt.savefig('scatter.png')\n",
    "    return\n",
    "\n",
    "def _graph_line(graph_name, LT_values):\n",
    "    x_axis = list(range(0, len(LT_values)))\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    plt.title(graph_name)\n",
    "    \n",
    "    for ltv in LT_values:\n",
    "        if len(ltv) > 3:\n",
    "            LT_v, x, label, lower, upper, c_lable = ltv\n",
    "            plt.plot(x, LT_v, label = label)\n",
    "            plt.fill_between(x, lower, upper, alpha=0.5, label = c_lable)\n",
    "        else:\n",
    "            LT_v, x, label = ltv\n",
    "            plt.plot(x, LT_v, label = label)\n",
    "                  \n",
    "    plt.xlabel('days')\n",
    "\n",
    "    plt.ylabel('LT50 values')\n",
    "    plt.legend(prop={'size': 20})\n",
    "    plt.savefig('linegraph.png')\n",
    "    return\n",
    "\n",
    "def confidence_region_95(pred_model):\n",
    "    std2 = pred_model.stddev.mul_(1.96)\n",
    "    mean = pred_model.mean\n",
    "    return mean.sub(std2), mean.add(std2)\n",
    "\n",
    "def run_and_plot(test_pred, val_idx, test_LT50_gt):\n",
    "    test_LT50_gt = test_LT50_gt.to(device)\n",
    "    output_mean = test_pred.mean.cpu().numpy()\n",
    "    # lower, upper = test_pred.confidence_region()\n",
    "    lower, upper = confidence_region_95(test_pred)\n",
    "    truth = test_LT50_gt[val_idx].cpu().numpy()[:, None]\n",
    "    print(output_mean.shape, test_furguson.shape, lower.shape, upper.shape)\n",
    "    print(\"mean loss:\", criterion(test_pred.mean[val_idx], test_LT50_gt[val_idx]))\n",
    "    print(\"upper loss:\", criterion(upper[val_idx], test_LT50_gt[val_idx]))\n",
    "    in_range = (test_LT50_gt[val_idx] < upper[val_idx]) * (test_LT50_gt[val_idx] > lower[val_idx])\n",
    "    print(in_range)\n",
    "    print(\"ratio of gt in confidence range: \", in_range.sum() / len(in_range))\n",
    "    print(\"ratio of under gt: \", (test_LT50_gt[val_idx] < upper[val_idx]).sum() / len(in_range))\n",
    "    # print(truth.shape)\n",
    "    LT50_prediction = output_mean[val_idx]\n",
    "    LT50_truth = truth[:, 0]\n",
    "    lower = lower.cpu()\n",
    "    upper = upper.cpu()\n",
    "    lower += test_furguson\n",
    "    upper += test_furguson\n",
    "    _graph_bar(\"Prediction vs Ground Truth (Difference bwt Hc and LT50 GT) Same day\", LT50_prediction, LT50_truth, )\n",
    "    _graph_scatter(\"Prediction vs Ground Truth (Difference bwt Hc and LT50 GT) Same day\", LT50_prediction, LT50_truth, )\n",
    "    _graph_bar_diff(\"GT - Prediction (Same Day)\", LT50_truth, LT50_prediction)\n",
    "    # print(test_furguson[val_last_idx].shape, test_pred.shape, truth.shape)\n",
    "    _graph_line(\"LT50 lines (Residual)\", [(test_furguson, np.arange(test_furguson.shape[0]), \"Furguson(with linear interp)\"), \\\n",
    "                               (output_mean + test_furguson, np.arange(output_mean.shape[0]), \"Pred_GPs_Mean\", lower.numpy(), upper.numpy(), \"Confidence Region(95% Interval)\"), \\\n",
    "                               (truth.flatten() + test_furguson[val_last_idx], val_last_idx, \"Ground Truth\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209ccc4-bd86-49ef-9d87-e1f3ff85536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    run_and_plot(gp_best_loss, val_last_idx, test_LT50_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2c7d0-5f5f-4c1d-b4b4-09fd6a9eb6f1",
   "metadata": {},
   "source": [
    "### Legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6482ae-8262-4331-9ae6-5cc4de06b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuable_idx = get_valuable_idx(y_test[:,:, 0])\n",
    "with torch.no_grad():\n",
    "    output_s, output_n, _= best_model(x_test.to(device))\n",
    "output = output_s[valuable_idx[0],valuable_idx[1]].cpu().numpy()\n",
    "truth = y_test[:,:, 0][valuable_idx[0],valuable_idx[1]].numpy()[:, None]\n",
    "# print(output.shape, truth.shape)\n",
    "print(criterion(output_s[valuable_idx[0],valuable_idx[1]], y_test[:,:, 0][valuable_idx[0],valuable_idx[1]][:, None].to(device)))\n",
    "print(output.shape)\n",
    "print(truth.shape)\n",
    "LT50_prediction = output[:, 0]\n",
    "LT50_truth = truth[:, 0]\n",
    "print(LT50_truth)\n",
    "print(LT50_prediction)\n",
    "_graph_bar(\"Prediction vs Ground Truth (Difference bwt Hc and LT50 GT) Same day\", LT50_truth, LT50_prediction)\n",
    "_graph_scatter(\"Prediction vs Ground Truth (Difference bwt Hc and LT50 GT) Same day\", LT50_truth, LT50_prediction)\n",
    "_graph_bar_diff(\"GT - Prediction (Next Day)\", LT50_truth, LT50_prediction)\n",
    "print(output.shape)\n",
    "_graph_line(\"LT50 lines\", [(test_furguson, np.arange(test_furguson.shape[0]), \"pred_hc(with linear interp)\"), \\\n",
    "                           (output.flatten() + test_furguson[val_last_idx], val_last_idx, \"pred_LT_rnn\"), \\\n",
    "                           (truth.flatten() + test_furguson[val_last_idx], val_last_idx, \"ground truth\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
